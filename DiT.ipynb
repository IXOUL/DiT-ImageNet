{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "958b1977",
   "metadata": {},
   "source": [
    "### Patchify\n",
    "cut 2D image $H*W*C$ ($H$ height, $W$ width, $C$ channel) into many patches, flattening the patch and project it into dimension $D$, getting a series of tokens for transformer to operate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a0bbd9",
   "metadata": {},
   "source": [
    "Code reference: \n",
    "\n",
    "I read the original paper about the logic of the Patchify in ViT, and searched for other people's work of writing it. Since the patch embedding logic is the same in DiT and ViT\n",
    "\n",
    "https://github.com/vballoli/vit-flax/blob/main/vit_flax/vit.py\n",
    "\n",
    "ChatGPT learning the functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6444f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    patch_size: int\n",
    "    dimension: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # x: shape (B, H, W, C)\n",
    "        B, H, W, C = x.shape\n",
    "        patches_h = H // self.patch_size\n",
    "        patches_w = W // self.patch_size\n",
    "        total_num_patches = patches_h * patches_w\n",
    "        \n",
    "        # 1. reshape the images into patch grids\n",
    "        # -> (B, patches_h, patch, patches_w, patch, C)\n",
    "        x = x.reshape(B, patches_h, self.patch_size, patches_w, self.patch_size, C)\n",
    "\n",
    "        # 2. transpose to bring patch dimensions together\n",
    "        # -> (B, patches_h, patches_w, patch, patch, C)\n",
    "        x = jnp.transpose(x, (0, 1, 3, 2, 4, 5))\n",
    "\n",
    "        # 3. flatten patch pixels -> (B, total_num_patches, patch_size*patch_size*C)\n",
    "        x = x.reshape(B, total_num_patches, self.patch_size * self.patch_size * C)\n",
    "\n",
    "        # 4. linear projection to \"dimension\" for Transformer\n",
    "        x = nn.Dense(self.dimension)(x)\n",
    "\n",
    "        # 5. learned positional embedding\n",
    "        pos = self.param(\"pos\", nn.initializers.normal(0.02), (1, total_num_patches, self.dim))\n",
    "        x = x + pos\n",
    "\n",
    "        return x, (patches_h, patches_w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b3b934",
   "metadata": {},
   "source": [
    "Reference code\n",
    "\n",
    "https://github.com/kelechi-c/dit_flow/tree/main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79462290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "102808c6",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dac39d7",
   "metadata": {},
   "source": [
    "implementing Transformer Block\n",
    "1. LayerNorm\n",
    "2. Multi-Head Self Attention\n",
    "3. Residual Connection\n",
    "4. LayerNorm + MLP\n",
    "5. Residual Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc18ce6b",
   "metadata": {},
   "source": [
    "### Multi-Head Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f832bc",
   "metadata": {},
   "source": [
    "Reference Code\n",
    "\n",
    "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial6/Transformers_and_MHAttention.html\n",
    "\n",
    "I learned how to write the Jax version for multihead attention (also learning some jax syntax such as implementing the scaled dot product using Jax) from this tutorial;\n",
    "I also used ChatGPT for helping me to clarify the meaning of the functions and some logic, as well as debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15c048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "\n",
    "class MultiHeadSelfAttenton(nn.Module):\n",
    "    dimension: int\n",
    "    num_heads: int\n",
    "    dropout: float = 0.0\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, mask=None, deterministic=True):\n",
    "        # x.shape = (batchsize, number of tokens, dimension)\n",
    "        # every token is a D-dimension vector, including the information of an image patch\n",
    "        B, N, D = x.shape\n",
    "        H = self.num_heads\n",
    "        head_dimension = D // H\n",
    "\n",
    "        assert D % H == 0, \"dimension must be divisible by number of heads\"\n",
    "\n",
    "        # QKV projection\n",
    "        qkv = nn.Dense(D * 3)(x)  # qkv.shape = (B, N, 3 * D)\n",
    "        qkv = qkv.reshape(B, N, 3, H, head_dimension)\n",
    "        qkv = qkv.transpose(2, 0, 3, 1, 4)  # qkv.shape = (3, B, H, N, head_dimension)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        values, attention = self.scaled_dot_product(q, k, v, mask=mask)\n",
    "        # Apply dropout to attention output\n",
    "        values = nn.Dropout(rate=self.dropout)(values, deterministic=deterministic)\n",
    "\n",
    "        # Concatenate heads and project back to D\n",
    "        values = values.transpose(0, 2, 1, 3).reshape(B, N, D)\n",
    "        output = nn.Dense(D)(values)\n",
    "\n",
    "        return output, attention\n",
    "\n",
    "    @staticmethod\n",
    "    def scaled_dot_product(q, k, v, mask=None):\n",
    "        # q, k, v shape = (B, H, N, head_dim)\n",
    "        d_k = q.shape[-1]\n",
    "\n",
    "        # Step 1: compute attention logits = Q @ K^T / sqrt(d_k)\n",
    "        attn_logits = jnp.matmul(q, jnp.swapaxes(k, -2, -1))\n",
    "        attn_logits = attn_logits / math.sqrt(d_k)\n",
    "\n",
    "        # Step 2: apply mask if provided (mask 0 = ignore)\n",
    "        if mask is not None:\n",
    "            attn_logits = jnp.where(mask == 0, -9e15, attn_logits)\n",
    "\n",
    "        # Step 3: softmax to get normalized attention weights\n",
    "        attention = nn.softmax(attn_logits, axis=-1)\n",
    "\n",
    "        # Step 4: weighted sum of values\n",
    "        values = jnp.matmul(attention, v)\n",
    "\n",
    "        return values, attention\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My_DiT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
